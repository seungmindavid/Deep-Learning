{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICV2OZZBngXy"
      },
      "source": [
        "# Positively-Toned Text Generator\n",
        "\n",
        "Transformer with Encoder Layers & Decoder Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCLGerg4pgY3",
        "outputId": "33ab131a-df83-45bc-a3bb-b9d57fd53222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "\n",
        "# General\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Device Setting\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-eDqtg0qa4F"
      },
      "source": [
        "### Import tokenizer from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "e370322303f14a4f988db4a0706cc64a",
            "a1818b7e6d5b4274b3aab52afa2d6b5e",
            "c6ed637ff9854abbaccad7681fafdf04",
            "b7e225094ced4d059e14dd8352ad52c0",
            "8015fabb22fb47da9f4ca79e50134a3e",
            "b2a125f819814fe69707a1bb023e3dd6",
            "c1fac369de064057873f4a0956320fc9",
            "8b14ead53fec4b4e976f19a886eda194",
            "766f01ee880b45d4b6b82507275aedb0",
            "e0c39cafc43245589f1143eb5cb4a58d",
            "9f3c23b017c342909083c6d6c014111d",
            "3a1c1e6f89e64aee879d7d174c2fbc98",
            "9b6665729ab54898b443ea6b7201dd5e",
            "e63d2c52fbe44fdf9d0eea5edd5c61ea",
            "2745096a7cf045fd8fc49798aa288b78",
            "d1bfc5a5876b40de9d87c7327305aaea",
            "b5f95ccb4a524c8491d6774164c298b6",
            "da48568fba184bbda81d738c70e864c7",
            "b4aff983cdc64f81996062c778a3899e",
            "78979ab4cc49432a8dc213738679c987",
            "7b69457bc34b499d8a7f457991a51102",
            "f04df418bda8492a9c16f4c36ccb21a1",
            "83e3ff65e15845a8993a97c798b70f46",
            "860ecadfe5db4c2091efc4639460800e",
            "05686f36ecd24d07bb33bab900d247ee",
            "3b30baca83534a80b6ea98aebb5f9d99",
            "19f49f0627614c709ddce7e3a7ece74e",
            "3681938753284dfcb499759c1f385a16",
            "8bc60ed2782c44dab8e4ee59461cfe98",
            "acf6f0c5d993451a97d3c11ec13b3aaa",
            "1a8e3bcc630a4af78664647b63cbb70d",
            "dea53186d23643c9991b5c4f2c0b8c11"
          ]
        },
        "id": "NzYauCwPqZfZ",
        "outputId": "d16485f3-4e59-4362-942e-c2e20d59973f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e370322303f14a4f988db4a0706cc64a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGL42XDEqje-",
        "outputId": "fc587b90-deaa-43bf-c0d9-c571666c0cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/510.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets tokenizers\n",
        "\n",
        "# Huggingface\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "b7H15y0kq4oN"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "\n",
        "def get_config(num_epochs, d_model, h, depth, vocab_size):\n",
        "  return{\n",
        "      \"train_batch_size\": 8,\n",
        "      \"test_batch_size\": 1,\n",
        "      \"num_epochs\": num_epochs,\n",
        "      \"lr\": 3e-4,\n",
        "      \"seq_len\": 256,\n",
        "      \"d_model\": d_model,\n",
        "      \"h\": h,\n",
        "      \"depth\" : depth,\n",
        "      \"dropout\": 0.2,\n",
        "      \"num_classes\": vocab_size,\n",
        "      \"checkpoint\": 'bert-base-uncased'\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFrZVWK6F5o8"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8XRKbpXrAU3",
        "outputId": "58ba96b1-a3a5-49b6-8db4-dd76b7f4173e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Generated by OpenAI API shape: (10559, 2)\n",
            "0    Please kindly refrain from distracting me whil...\n",
            "1    It seems like there was some misunderstanding ...\n",
            "2    How about: \"Please remember to always communic...\n",
            "3    Seize the day and enjoy yourself. Indulge in y...\n",
            "4    It seems like you may be having some issues wi...\n",
            "Name: reframed_text, dtype: object\n",
            "Error code: 400 occured on 37 indices\n",
            "After dropping errors: (10522, 2)\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv('toxic_reframed.csv')\n",
        "\n",
        "print(f\"Data Generated by OpenAI API shape: {dataset.shape}\")\n",
        "print(dataset['reframed_text'].head())\n",
        "\n",
        "errors = dataset[dataset['reframed_text'].isna()].index\n",
        "errors = errors.tolist()\n",
        "print(f\"Error code: 400 occured on {len(errors)} indices\")\n",
        "\n",
        "dataset = dataset.dropna()\n",
        "print(f\"After dropping errors: {dataset.shape}\")\n",
        "\n",
        "X = dataset['comment_text']\n",
        "y = dataset['reframed_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fIjf5U7ndKD",
        "outputId": "306a7abc-a021-4ea4-e73b-8c505502f03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-trained Tokenizer below: \n",
            " BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "Target: ['Please kindly refrain from distracting me while I am working. Thank you for your understanding.', \"It seems like there was some misunderstanding in your previous comment about the historical events of World War II and the Holocaust. It's important to remember the tragic loss of lives, especially the brutal slaying of Jewish people, during that time.\\n\\nInstead of focusing on negative assumptions or stereotypes, let's embrace understanding and respect for all individuals regardless of their beliefs or backgrounds. Building a compassionate and inclusive community is essential for fostering positive interactions and learning from one another.\\n\\nLet's all strive to promote kindness and acceptance in our conversations, and refrain from using hurtful language or making derogatory remarks towards others. By spreading love and empathy, we can create a more harmonious environment for everyone to share their thoughts and perspectives.\", 'How about: \"Please remember to always communicate respectfully and kindly, even in moments of frustration. It\\'s important to choose our words carefully and maintain a positive attitude.\"', 'Seize the day and enjoy yourself. Indulge in your favorite drink and embrace the moment of letting loose. Cheers to feeling great and having a wonderful time right now!', \"It seems like you may be having some issues with the deletions of your content. Please know that I am here to help address any concerns you have and find a resolution together in a respectful manner. Let's work together to ensure your contributions are valued and respected.\"]\n",
            "Source: Input Ids: tensor([[  101, 10338,  6342,  9102,  2077,  2017, 18138,  2105,  2006,  2026,\n",
            "          2147,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  2017,  2024,  5637,  2030,  3424,  3366,  7382, 29050,  2078,\n",
            "          1029, 28185,  2317,  6816,  2033,  5004,   999, 14806,  4095, 23644,\n",
            "           999,  7910,  1010,  2045,  2024,  2048,  3971,  1010,  2339,  2017,\n",
            "          2079, 23516,  2026,  7615,  2055,  1059,  2860,  2475,  1010,  2008,\n",
            "         11513,  2001, 23197, 22889,  4710,  2075,  1997,  5181,  1998,  2025,\n",
            "          5637,  2015,  1013, 16006,  2015,  1013, 22889, 29553,  1013,  3087,\n",
            "          1012,  1012,  1012,  1015,  1011,  2065,  2017,  2024,  3424,  1011,\n",
            "          4100, 10772,  1010,  2084, 27545,  2115,  2132, 13852,  1998,  2175,\n",
            "          2000,  1996,  3096,  4974,  6295,   999,  1016,  1011,  2065,  2017,\n",
            "          4797,  2616,  1997,  1996,  6331,  1010,  2008, 15949,  2003,  1037,\n",
            "          9252,  8254,  1010,  2191,  1037,  7279, 23091, 11937,  3406,  2080,\n",
            "          2006,  2115,  6130,  2175,  2000,  1996, 16795,  6553, 11678,  2007,\n",
            "          2115,  5637, 14412,  2015,   999,  1017,  1011,  2034,  1998,  2197,\n",
            "          5432,  1010,  2017,  8239,  5637,  1011,  1045,  2180,  1005,  1056,\n",
            "          9120,  2065,  2151,  2062,  6394, 14021, 21547,  2078,  2052,  4339,\n",
            "          1999,  2026,  3931,   999,  1045,  2123,  1005,  1056,  4299,  2000,\n",
            "          2831,  2000,  2017,  4902,   999,  2022,  8059,  1997,  1996,  2601,\n",
            "          2217,   999,   102],\n",
            "        [  101,  6616,  2115, 18294,  2388,  1999,  1996,  4632,  1010,  4318,\n",
            "           999,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  2131, 21746,  2039,  1012,  2131,  6616,  4402,  2098,  2039,\n",
            "          1012,  2288,  1037,  4392,  2008,  2017,  2064,  2102,  2404,  2091,\n",
            "          1029,  1029,  1029,  1013,  2131,  6616,  2039,  2131, 21746,  2039,\n",
            "          1012,  1045,  1005,  1049, 21746,  2039,  2157,  2085,   999,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  5236,  3521,  1997,  4485,  2644,  3972, 20624,  3070,  2026,\n",
            "          4933, 22052,  2175,  3280,  1998,  2991,  1999,  1037,  4920,  2175,\n",
            "          2000,  3109,   999,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]])\n",
            "Source: Token type Ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]])\n",
            "Source: Attention masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]])\n",
            "Target: Input Ids: tensor([[  101, 10338,  6342,  9102,  2077,  2017, 18138,  2105,  2006,  2026,\n",
            "          2147,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  2017,  2024,  5637,  2030,  3424,  3366,  7382, 29050,  2078,\n",
            "          1029, 28185,  2317,  6816,  2033,  5004,   999, 14806,  4095, 23644,\n",
            "           999,  7910,  1010,  2045,  2024,  2048,  3971,  1010,  2339,  2017,\n",
            "          2079, 23516,  2026,  7615,  2055,  1059,  2860,  2475,  1010,  2008,\n",
            "         11513,  2001, 23197, 22889,  4710,  2075,  1997,  5181,  1998,  2025,\n",
            "          5637,  2015,  1013, 16006,  2015,  1013, 22889, 29553,  1013,  3087,\n",
            "          1012,  1012,  1012,  1015,  1011,  2065,  2017,  2024,  3424,  1011,\n",
            "          4100, 10772,  1010,  2084, 27545,  2115,  2132, 13852,  1998,  2175,\n",
            "          2000,  1996,  3096,  4974,  6295,   999,  1016,  1011,  2065,  2017,\n",
            "          4797,  2616,  1997,  1996,  6331,  1010,  2008, 15949,  2003,  1037,\n",
            "          9252,  8254,  1010,  2191,  1037,  7279, 23091, 11937,  3406,  2080,\n",
            "          2006,  2115,  6130,  2175,  2000,  1996, 16795,  6553, 11678,  2007,\n",
            "          2115,  5637, 14412,  2015,   999,  1017,  1011,  2034,  1998,  2197,\n",
            "          5432,  1010,  2017,  8239,  5637,  1011,  1045,  2180,  1005,  1056,\n",
            "          9120,  2065,  2151,  2062,  6394, 14021, 21547,  2078,  2052,  4339,\n",
            "          1999,  2026,  3931,   999,  1045,  2123,  1005,  1056,  4299,  2000,\n",
            "          2831,  2000,  2017,  4902,   999,  2022,  8059,  1997,  1996,  2601,\n",
            "          2217,   999,   102],\n",
            "        [  101,  6616,  2115, 18294,  2388,  1999,  1996,  4632,  1010,  4318,\n",
            "           999,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  2131, 21746,  2039,  1012,  2131,  6616,  4402,  2098,  2039,\n",
            "          1012,  2288,  1037,  4392,  2008,  2017,  2064,  2102,  2404,  2091,\n",
            "          1029,  1029,  1029,  1013,  2131,  6616,  2039,  2131, 21746,  2039,\n",
            "          1012,  1045,  1005,  1049, 21746,  2039,  2157,  2085,   999,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  5236,  3521,  1997,  4485,  2644,  3972, 20624,  3070,  2026,\n",
            "          4933, 22052,  2175,  3280,  1998,  2991,  1999,  1037,  4920,  2175,\n",
            "          2000,  3109,   999,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]])\n",
            "Target: Token type Ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]])\n",
            "Target: Attention masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "vocab_size = tokenizer.vocab_size\n",
        "config = get_config(5, 256, 64, 3, vocab_size) #(num_epochs, d_model, h, depth, vocab_size)\n",
        "\n",
        "print(f'Pre-trained Tokenizer below: \\n {tokenizer}')\n",
        "\n",
        "source = list(X[:5].values)\n",
        "target = list(y[:5].values)\n",
        "\n",
        "print(f'Target: {target}')\n",
        "\n",
        "encoded_sources = tokenizer(source, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_targets = tokenizer(target, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "src_input_ids = encoded_sources['input_ids']\n",
        "src_token_type_ids = encoded_sources['token_type_ids']\n",
        "src_attention_mask = encoded_sources['attention_mask']\n",
        "\n",
        "print(f'Source: Input Ids: {src_input_ids}')\n",
        "print(f'Source: Token type Ids: {src_token_type_ids}')\n",
        "print(f'Source: Attention masks: {src_attention_mask}')\n",
        "\n",
        "tgt_input_ids = encoded_sources['input_ids']\n",
        "tgt_token_type_ids = encoded_sources['token_type_ids']\n",
        "tgt_attention_mask = encoded_sources['attention_mask']\n",
        "\n",
        "print(f'Target: Input Ids: {tgt_input_ids}')\n",
        "print(f'Target: Token type Ids: {tgt_token_type_ids}')\n",
        "print(f'Target: Attention masks: {tgt_attention_mask}')\n",
        "\n",
        "\n",
        "# Decode\n",
        "# tokenizer.decode() is designed to work with a single sequence at a time.\n",
        "src_list = src_input_ids.tolist()\n",
        "decoded_texts = [tokenizer.decode(ids) for ids in src_list]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "pQOWyqt-ngkJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PositiveDataset(Dataset):\n",
        "  def __init__(self, source, target, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.txt = source.values\n",
        "    self.target = target.values\n",
        "\n",
        "    self.seq_len = config['seq_len']\n",
        "    self.h = config['h']\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(config['checkpoint'])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.txt)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    source = str(self.txt[idx])\n",
        "    target = str(self.target[idx])\n",
        "\n",
        "    h = self.h\n",
        "    seq_len = self.seq_len\n",
        "\n",
        "    # tokenized_source\n",
        "    tokenized_source = self.tokenizer(source,\n",
        "                                     max_length = self.seq_len,\n",
        "                                     padding='max_length',\n",
        "                                     truncation = True\n",
        "                                     )\n",
        "\n",
        "    # encoder inputs\n",
        "    encoder_input = torch.tensor(tokenized_source['input_ids'], dtype= torch.long)\n",
        "\n",
        "    # source masks\n",
        "    encoder_mask = torch.tensor(tokenized_source['attention_mask'], dtype= torch.long).unsqueeze(0)\n",
        "    encoder_mask = encoder_mask.repeat(1, h, 1)\n",
        "    encoder_mask = encoder_mask.expand(seq_len, h, seq_len)\n",
        "    encoder_mask = encoder_mask.transpose(0,1).contiguous()\n",
        "\n",
        "    # tokenized_target\n",
        "    tokenized_target = self.tokenizer(target,\n",
        "                                     max_length = self.seq_len,\n",
        "                                     padding='max_length',\n",
        "                                     truncation = True\n",
        "                                     )\n",
        "\n",
        "    # decoder inputs (should not be used in loss function. It's an input)\n",
        "    decoder_input = torch.tensor(tokenized_target['input_ids'], dtype= torch.long)\n",
        "    # remove special tokens for end of statement\n",
        "    decoder_input = torch.where(decoder_input == 102, torch.tensor(0), decoder_input)\n",
        "\n",
        "    # target_masks\n",
        "    decoder_mask = torch.tensor(tokenized_target['attention_mask'], dtype= torch.long).unsqueeze(0)\n",
        "    decoder_mask = decoder_mask.repeat(1, h, 1)\n",
        "    decoder_mask = decoder_mask.expand(seq_len, h, seq_len)\n",
        "    decoder_mask = decoder_mask.transpose(0,1).contiguous().type(torch.int)\n",
        "    # target_masks must be causally masked\n",
        "    decoder_mask = decoder_mask & torch.tril(torch.ones(h, seq_len, seq_len)).type(torch.int)\n",
        "\n",
        "    # label (should be used in loss function)\n",
        "    label = torch.tensor(tokenized_target['input_ids'], dtype= torch.long)\n",
        "    # remove special tokens for start of statement\n",
        "    label = torch.cat((label[1:], torch.tensor([0])))\n",
        "\n",
        "\n",
        "    item = {\n",
        "          'source_txt': source,\n",
        "          'target_txt': target,\n",
        "          'encoder_input': encoder_input,\n",
        "          'encoder_mask': encoder_mask,\n",
        "          'decoder_input': decoder_input,\n",
        "          'decoder_mask': decoder_mask,\n",
        "          'label' : label\n",
        "    }\n",
        "\n",
        "    return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkvaQK3o07gY"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QwyOVkn0z6e",
        "outputId": "808ee24f-5d81-4777-8c75-6bc0fbc04162"
      },
      "outputs": [],
      "source": [
        "config = get_config(5, 256, 64, 3, vocab_size) #(num_epochs, d_model, h, depth, vocab_size)\n",
        "\n",
        "source = X\n",
        "target = y\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    PositiveDataset(source, target, config),\n",
        "    batch_size = config['train_batch_size'],\n",
        "    shuffle=True,\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "batch = next(dataiter)\n",
        "\n",
        "for idx in range(len(batch['encoder_input'])):\n",
        "  #print(f\"source text: {batch['source_txt'][idx]}\\n\")\n",
        "  print(f\"target text: {batch['target_txt'][idx]}\\n\")\n",
        "  print(f\"encoder input: {batch['encoder_input'][idx]}\\n\")\n",
        "  print(f\"encoder mask: {batch['encoder_mask'].size()}\\n\")\n",
        "  print(f\"decoder input: {batch['decoder_input'][idx]}\\n\")\n",
        "  print(f\"decoder mask: {batch['decoder_mask'].size()}\\n\")\n",
        "  print(f\"label: {batch['label'][idx]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEUa71FaIZM7"
      },
      "source": [
        "## Input Embedding & Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "rUWz0IWu1Kk4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * (self.d_model ** 0.5)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self, d_model, seq_len):\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "    pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n",
        "\n",
        "    pe[:,0::2] = torch.sin(pos*div)\n",
        "    pe[:,1::2] = torch.cos(pos*div)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18BHNE3rIVxo"
      },
      "source": [
        "### Check dimension for input embedding and positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGlf-n703TL4",
        "outputId": "a2356e0c-71a1-4bf0-ff5f-0f92578e6d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "Source ids: tensor([  101,  2017,  2024,  5637,  2030,  3424,  3366,  7382, 29050,  2078,\n",
            "         1029, 28185,  2317,  6816,  2033,  5004,   999, 14806,  4095, 23644,\n",
            "          999,  7910,  1010,  2045,  2024,  2048,  3971,  1010,  2339,  2017,\n",
            "         2079, 23516,  2026,  7615,  2055,  1059,  2860,  2475,  1010,  2008,\n",
            "        11513,  2001, 23197, 22889,  4710,  2075,  1997,  5181,  1998,  2025,\n",
            "         5637,  2015,  1013, 16006,  2015,  1013, 22889, 29553,  1013,  3087,\n",
            "         1012,  1012,  1012,  1015,  1011,  2065,  2017,  2024,  3424,  1011,\n",
            "         4100, 10772,  1010,  2084, 27545,  2115,  2132, 13852,  1998,  2175,\n",
            "         2000,  1996,  3096,  4974,  6295,   999,  1016,  1011,  2065,  2017,\n",
            "         4797,  2616,  1997,  1996,  6331,  1010,  2008, 15949,  2003,  1037,\n",
            "         9252,  8254,  1010,  2191,  1037,  7279, 23091, 11937,  3406,  2080,\n",
            "         2006,  2115,  6130,  2175,  2000,  1996, 16795,  6553, 11678,  2007,\n",
            "         2115,  5637, 14412,  2015,   999,  1017,  1011,  2034,  1998,  2197,\n",
            "         5432,  1010,  2017,  8239,  5637,  1011,  1045,  2180,  1005,  1056,\n",
            "         9120,  2065,  2151,  2062,  6394, 14021, 21547,  2078,  2052,  4339,\n",
            "         1999,  2026,  3931,   999,  1045,  2123,  1005,  1056,  4299,  2000,\n",
            "         2831,  2000,  2017,  4902,   999,  2022,  8059,  1997,  1996,  2601,\n",
            "         2217,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n",
            "Source ids size: torch.Size([256])\n",
            "\n",
            "Source after embedded size: torch.Size([1, 256, 256])\n",
            "Source after positional embedded size: torch.Size([1, 256, 256])\n",
            "\n",
            "Source after positional embedding: tensor([[[  3.8187,  15.3369,  25.8818,  ...,   9.1183,  23.2792,  14.6896],\n",
            "         [ 18.6872, -12.2074,  33.5976,  ...,  10.6843,  24.8900,  24.4811],\n",
            "         [ -5.9145, -11.6896, -17.0123,  ..., -16.1004,  26.1636,  23.9937],\n",
            "         ...,\n",
            "         [-26.0420,  17.0719, -35.9504,  ...,  18.8926,   0.9942,  16.7879],\n",
            "         [-26.5848,  16.2815, -36.8126,  ...,  18.8926,   0.9943,  16.7879],\n",
            "         [-27.5432,  16.3112, -37.1285,  ...,  18.8926,   0.9944,  16.7879]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "Target after embedded size: torch.Size([1, 256, 256])\n",
            "Target after positional embedded size: torch.Size([1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(config['checkpoint'])\n",
        "print(tokenizer)\n",
        "\n",
        "input_embedding = InputEmbedding(tokenizer.vocab_size, config['d_model'])\n",
        "positional_encoding = PositionalEmbedding(config['d_model'], config['seq_len'])\n",
        "\n",
        "\n",
        "source = X\n",
        "target = y\n",
        "\n",
        "train_ds = PositiveDataset(source, target, config)\n",
        "data = train_ds.__getitem__(1)\n",
        "\n",
        "encoder_input = data['encoder_input']\n",
        "decoder_input = data['decoder_input']\n",
        "\n",
        "print(f\"Source ids: {encoder_input}\")\n",
        "print(f\"Source ids size: {encoder_input.size()}\")\n",
        "\n",
        "\n",
        "# train_ds.__getitem__(1)['source_ids'].unsqueeze(0) is needed only here.\n",
        "# because when we use dataloader, it will return size ([1, 256]), not ([256])\n",
        "src_emb = input_embedding(encoder_input.unsqueeze(0))\n",
        "print(f\"\\nSource after embedded size: {src_emb.size()}\")\n",
        "src_pe = positional_encoding(src_emb)\n",
        "print(f\"Source after positional embedded size: {src_pe.size()}\\n\")\n",
        "print(f\"Source after positional embedding: {src_pe}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "tgt_emb = input_embedding(decoder_input.unsqueeze(0))\n",
        "print(f\"Target after embedded size: {tgt_emb.size()}\")\n",
        "tgt_pe = positional_encoding(tgt_emb)\n",
        "print(f\"Target after positional embedded size: {tgt_pe.size()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW4TAZ8bIqRl"
      },
      "source": [
        "## Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "wgtFoeMm3x6p"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.d_model = config['d_model']\n",
        "    self.d_ff = self.d_model * 4\n",
        "\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(self.d_model, self.d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.d_ff, self.d_model),\n",
        "        nn.Dropout(config['dropout'])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ffn(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqhvkuInIr1s"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "VM9dmI40nmTf"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.d_model = config['d_model']\n",
        "    self.h = config['h']\n",
        "\n",
        "    assert self.d_model % self.h == 0, \"d_model must be divisible by h\"\n",
        "\n",
        "\n",
        "    self.d_k = self.d_model // self.h\n",
        "    # Query, key, value\n",
        "    self.W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "    self.W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "    self.W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    # Last Layer\n",
        "    self.W_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    # dropout\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "\n",
        "    h = self.h\n",
        "    d_k = self.d_k\n",
        "    d_model = self.d_model\n",
        "\n",
        "    query = self.W_q(q)\n",
        "    key = self.W_k(k)\n",
        "    value = self.W_v(v)\n",
        "\n",
        "    # It's necessary since decode process will be (target, src_output, src_output, mask)\n",
        "    # which means, q_seq_len and k_seq_len will be different.\n",
        "    q_B, q_seq_len, _ = query.size()\n",
        "    k_B, k_seq_len, _ = key.size()\n",
        "    v_B, v_seq_len, _ = value.size()\n",
        "    # size: (Batch, Seq_len, d_model) -> (Batch, Seq_len, h, d_model // h)\n",
        "    query = query.view(q_B, q_seq_len, h, d_k)\n",
        "    key = key.view(k_B, k_seq_len, h, d_k)\n",
        "    value = value.view(v_B, v_seq_len, h, d_k)\n",
        "\n",
        "    # (Batch, Seq_len, h, d_k) -> (Batch, h, Seq_len, d_k)\n",
        "    # (Batch, h, Seq_len, d_k) -> (Batch * h, Seq_len, d_k)\n",
        "    query = query.transpose(1,2).contiguous().view(q_B * h, q_seq_len, d_k)\n",
        "    key = key.transpose(1,2).contiguous().view(k_B * h, k_seq_len, d_k)\n",
        "    value = value.transpose(1,2).contiguous().view(v_B * h, v_seq_len, d_k)\n",
        "\n",
        "    # Attention: W\n",
        "    # paying attention to each sequences, therefore size should be (Batch *h, Seq_len, Seq_len)\n",
        "    W = query @ key.transpose(1,2)\n",
        "    W = W / (d_model ** 0.5)\n",
        "\n",
        "    # If there is a mask, make masked spots -INF\n",
        "    # seq_len must be equal to query's sequence length.\n",
        "    if mask is not None:\n",
        "      mask = mask.view(k_B * h, k_seq_len, k_seq_len) # (B, h, Seq_len, Seq_len) => (B * h, Seq_len, Seq_len)\n",
        "      # It's for when we are generating new positive-toned manner text from the empty decoder_input\n",
        "      if q_seq_len != k_seq_len:\n",
        "        mask = mask[:,:q_seq_len,:]\n",
        "      W = W.masked_fill_(mask == 0, float('-inf'))\n",
        "\n",
        "    W = W.softmax(dim = -1)\n",
        "    # drop out\n",
        "    W = self.dropout(W)\n",
        "\n",
        "    out = W @ value # (B * h, seq_len, d_k)\n",
        "    B, Seq_len, d_k = out.size()\n",
        "    B = B // h\n",
        "    out = out.view(B, h, Seq_len, d_k)\n",
        "    out = out.transpose(1,2).contiguous().view(B, Seq_len, h * d_k)\n",
        "    return self.W_o(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM5dEmnW77Vz"
      },
      "source": [
        "## Encoder & Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "qG-VF7VO763g"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "\n",
        "    self.d_model = config['d_model']\n",
        "\n",
        "    self.MultiHeadAttention = MultiHeadAttention(config)\n",
        "\n",
        "    self.ln_1 = nn.LayerNorm(self.d_model)\n",
        "    self.ln_2 = nn.LayerNorm(self.d_model)\n",
        "    self.FeedForward = FeedForward(config)\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = x + self.MultiHeadAttention(x, x, x, src_mask)\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.FeedForward(x)\n",
        "    x = self.ln_2(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "Qn0TPhxD8Maq"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.depth = config['depth']\n",
        "    self.blocks = nn.ModuleList([\n",
        "        EncoderBlock(config) for _ in range(self.depth)\n",
        "    ])\n",
        "    self.blocks = nn.Sequential(*self.blocks)\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    for block in self.blocks:\n",
        "      x = block(x, src_mask)\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "6LlgaKVXnoG7"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.d_model = config['d_model']\n",
        "\n",
        "    self.SelfHeadAttention = MultiHeadAttention(config)\n",
        "    self.CrossHeadAttention = MultiHeadAttention(config)\n",
        "\n",
        "    self.ln_1 = nn.LayerNorm(self.d_model)\n",
        "    self.ln_2 = nn.LayerNorm(self.d_model)\n",
        "    self.ln_3 = nn.LayerNorm(self.d_model)\n",
        "\n",
        "    self.FeedForward = FeedForward(config)\n",
        "\n",
        "  def forward(self, x, encoder_out, src_mask, tgt_mask):\n",
        "    # x: target, in our case positively-toned comment\n",
        "    x = x + self.SelfHeadAttention(x, x, x, tgt_mask)\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.CrossHeadAttention(x, encoder_out, encoder_out, src_mask)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.FeedForward(x)\n",
        "    x = self.ln_3(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "2aYoreACnpgp"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.depth = config['depth']\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        DecoderBlock(config) for _ in range(self.depth)\n",
        "    ])\n",
        "    self.blocks = nn.Sequential(*self.blocks)\n",
        "\n",
        "  def forward(self, x, encoder_out, src_mask, tgt_mask):\n",
        "    for block in self.blocks:\n",
        "      x = block(x, encoder_out, src_mask, tgt_mask)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_CGyW7V9laz"
      },
      "source": [
        "# Positive Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "QpI7H21j9iB0"
      },
      "outputs": [],
      "source": [
        "class PositiveGenerator(nn.Module):\n",
        "  def __init__(self, config, vocab_size):\n",
        "    super(PositiveGenerator, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(config)\n",
        "    self.decoder = Decoder(config)\n",
        "\n",
        "    self.d_model = config['d_model']\n",
        "    self.seq_len = config['seq_len']\n",
        "\n",
        "    # Input Embedding for source and target\n",
        "    self.src_embedding = InputEmbedding(vocab_size, self.d_model)\n",
        "    self.tgt_embedding = InputEmbedding(vocab_size, self.d_model)\n",
        "\n",
        "    # Positional Embedding for source and target\n",
        "    self.src_pos_embedding = PositionalEmbedding(self.d_model, self.seq_len)\n",
        "    self.tgt_pos_embedding = PositionalEmbedding(self.d_model, self.seq_len)\n",
        "\n",
        "    self.norm = nn.LayerNorm(self.d_model)\n",
        "    self.projection = nn.Linear(self.d_model, vocab_size)\n",
        "\n",
        "  def encode(self, source, src_mask):\n",
        "    source = self.src_embedding(source)\n",
        "    source = self.src_pos_embedding(source)\n",
        "    return self.encoder(source, src_mask)\n",
        "\n",
        "  def decode(self, target, encoder_out, src_mask, tgt_mask):\n",
        "    target = self.tgt_embedding(target)\n",
        "    target = self.tgt_pos_embedding(target)\n",
        "    return self.decoder(target, encoder_out, src_mask, tgt_mask)\n",
        "\n",
        "  def forward(self, decoder_out):\n",
        "    #out = self.norm(decoder_out)\n",
        "    out = self.projection(decoder_out)\n",
        "    return torch.log_softmax(out, dim=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd9IIkxC7rCA"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vwrLO5JzT_V_",
        "outputId": "9d2db86e-98dd-4586-feff-9591a9e47aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_batch_size': 8, 'test_batch_size': 1, 'num_epochs': 5, 'lr': 0.0003, 'seq_len': 256, 'd_model': 256, 'h': 64, 'depth': 3, 'dropout': 0.2, 'num_classes': 30522, 'checkpoint': 'bert-base-uncased'}\n",
            "PositiveGenerator(\n",
            "  (encoder): Encoder(\n",
            "    (blocks): Sequential(\n",
            "      (0): EncoderBlock(\n",
            "        (MultiHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (FeedForward): FeedForward(\n",
            "          (ffn): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): EncoderBlock(\n",
            "        (MultiHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (FeedForward): FeedForward(\n",
            "          (ffn): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): EncoderBlock(\n",
            "        (MultiHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (FeedForward): FeedForward(\n",
            "          (ffn): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (blocks): Sequential(\n",
            "      (0): DecoderBlock(\n",
            "        (SelfHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (CrossHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (FeedForward): FeedForward(\n",
            "          (ffn): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (SelfHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (CrossHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (FeedForward): FeedForward(\n",
            "          (ffn): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): DecoderBlock(\n",
            "        (SelfHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (CrossHeadAttention): MultiHeadAttention(\n",
            "          (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (W_o): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (dropout): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (FeedForward): FeedForward(\n",
            "          (ffn): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (src_embedding): InputEmbedding(\n",
            "    (embedding): Embedding(30522, 256)\n",
            "  )\n",
            "  (tgt_embedding): InputEmbedding(\n",
            "    (embedding): Embedding(30522, 256)\n",
            "  )\n",
            "  (src_pos_embedding): PositionalEmbedding()\n",
            "  (tgt_pos_embedding): PositionalEmbedding()\n",
            "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (projection): Linear(in_features=256, out_features=30522, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 00: 100%|██████████| 1316/1316 [10:56<00:00,  2.00it/s, loss=0.184]\n",
            "Processing epoch 01: 100%|██████████| 1316/1316 [10:57<00:00,  2.00it/s, loss=0.161]\n",
            "Processing epoch 02: 100%|██████████| 1316/1316 [11:01<00:00,  1.99it/s, loss=0.388]\n",
            "Processing epoch 03: 100%|██████████| 1316/1316 [11:04<00:00,  1.98it/s, loss=0.778]\n",
            "Processing epoch 04: 100%|██████████| 1316/1316 [11:02<00:00,  1.99it/s, loss=0.370]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# path for saving model\n",
        "path = \"./PositiveGenerator\"\n",
        "pathlib.Path(f\"./{path}/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['checkpoint'])\n",
        "\n",
        "# configuration\n",
        "vocab_size = tokenizer.vocab_size\n",
        "config = get_config(5, 256, 64, 3, vocab_size) #(num_epochs, d_model, h, depth, vocab_size)\n",
        "print(config)\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "lr = config['lr']\n",
        "num_epochs = config['num_epochs']\n",
        "\n",
        "# model, optim, loss (cross entropy loss)\n",
        "model = PositiveGenerator(config, vocab_size).to(device)\n",
        "print(model)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "Loss = []\n",
        "# training\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  batch_iterator = tqdm(train_loader, desc = f'Processing epoch {epoch:02d}')\n",
        "  for batch in batch_iterator:\n",
        "    # for encoder\n",
        "    encoder_input = batch['encoder_input'].to(device)\n",
        "    encoder_mask = batch['encoder_mask'].to(device)\n",
        "    # for decoder\n",
        "    decoder_input = batch['decoder_input'].to(device)\n",
        "    decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "    # label for loss function\n",
        "    label = batch['label'].to(device)\n",
        "\n",
        "    B, seq_len = encoder_input.size()\n",
        "\n",
        "    # forward pass\n",
        "    # def encode(self, source, src_mask):\n",
        "    encoder_out = model.encode(encoder_input, encoder_mask)\n",
        "    # def decode(self, target, encoder_out, src_mask, tgt_mask):\n",
        "    decoder_out = model.decode(decoder_input, encoder_out, encoder_mask, decoder_mask)\n",
        "    out = model(decoder_out) # size:(Batch, Seq_len, tgt_vocab_size)\n",
        "\n",
        "    out = out.view(B*seq_len, vocab_size)\n",
        "    label = label.view(B*seq_len)\n",
        "\n",
        "    loss = ce_loss(out, label)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_iterator.set_postfix(loss=f\"{loss.item():6.3f}\")\n",
        "    Loss.append(loss.item())\n",
        "  torch.save(model.state_dict(), f'{path}/{epoch}.pth')\n",
        "\n",
        "torch.save(model.state_dict(), f'{path}/pg_final_model_3_64_512.pth')\n",
        "\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(Loss, linestyle='-')\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLAPmbdrYbAL"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D7i8vHiYcCV"
      },
      "outputs": [],
      "source": [
        "#validation\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "batch = next(dataiter)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  encoder_input = batch['encoder_input'].to(device)\n",
        "  encoder_mask = batch['encoder_mask'].to(device)\n",
        "  # for decoder\n",
        "  decoder_input = batch['decoder_input'].to(device)\n",
        "  decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "  # label for loss function\n",
        "  label = batch['label'].to(device)\n",
        "\n",
        "  B, seq_len = encoder_input.size()\n",
        "\n",
        "  # forward pass\n",
        "  # def encode(self, source, src_mask):\n",
        "  encoder_out = model.encode(encoder_input, encoder_mask)\n",
        "  # def decode(self, target, encoder_out, src_mask, tgt_mask):\n",
        "  decoder_out = model.decode(decoder_input, encoder_out, encoder_mask, decoder_mask)\n",
        "  out = model(decoder_out) # size:(Batch, Seq_len, tgt_vocab_size)\n",
        "  out = out.view(B*seq_len, vocab_size)\n",
        "\n",
        "  label = label.view(B*seq_len)\n",
        "  pred = torch.max(out, dim=-1).indices\n",
        "\n",
        "  txt_pred = tokenizer.decode(pred).replace(\"[PAD]\", \"\")\n",
        "  print(f\"\\nOriginal comments: {batch['source_txt']}\\n\")\n",
        "  print(f\"Generated comments: {txt_pred}\\n\")\n",
        "  print(f\"Target comments: {batch['target_txt']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3cw6QIClvWc"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQR_o5FSr11X",
        "outputId": "f38da03d-1a7a-4e8f-a49b-66a16713e4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of heads: 32\n",
            "Sequence length: 256\n",
            "\n",
            "tensor([[  101,  2644, 10086,  3436,  2000,  2191, 16542,  2592,  2006, 16948,\n",
            "           999,   999,   999,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]], device='cuda:0')\n",
            "Source mask size: torch.Size([1, 32, 256, 256])\n",
            "Source size: torch.Size([1, 256])\n",
            "Encoder output size: torch.Size([1, 256, 512])\n",
            "['[CLS] please consider removing the following wikipedia by following the guidelines for a more reliable source of information. thank you for your understanding. [SEP]']\n"
          ]
        }
      ],
      "source": [
        "config = get_config(5, 512, 32, 3, vocab_size) #(num_epochs, d_model, h, depth, vocab_size)\n",
        "\n",
        "pg_model = PositiveGenerator(config, vocab_size).to(device)\n",
        "pg_model.load_state_dict(torch.load(f'{path}/pg_final_model_3_32_512.pth',map_location=device))\n",
        "\n",
        "# config\n",
        "seq_len = config['seq_len']\n",
        "h = config['h']\n",
        "\n",
        "print(f\"\\nNumber of heads: {h}\")\n",
        "print(f\"Sequence length: {seq_len}\\n\")\n",
        "\n",
        "tokenized_source = tokenizer(\"stop editting to make incorrect information on wikipedia!!!\", max_length = seq_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "# source id\n",
        "encoder_input = torch.tensor(tokenized_source['input_ids'], dtype=torch.long).to(device)\n",
        "print(encoder_input)\n",
        "# source mask\n",
        "encoder_mask = torch.tensor(tokenized_source['attention_mask'], dtype= torch.long).unsqueeze(0).to(device)\n",
        "encoder_mask = encoder_mask.repeat(1, h, 1)\n",
        "encoder_mask = encoder_mask.expand(seq_len, h, seq_len)\n",
        "encoder_mask = encoder_mask.transpose(0,1).contiguous()\n",
        "# need unsqueeze here since we're not loading from dataloader\n",
        "encoder_mask = encoder_mask.unsqueeze(0)\n",
        "\n",
        "print(f\"Source mask size: {encoder_mask.size()}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  pg_model.eval()\n",
        "\n",
        "  print(f\"Source size: {encoder_input.size()}\")\n",
        "  encoder_output = pg_model.encode(encoder_input, encoder_mask)\n",
        "  print(f\"Encoder output size: {encoder_output.size()}\")\n",
        "\n",
        "  decoder_input = torch.empty(1,1).fill_(101).type_as(encoder_input).to(device)\n",
        "\n",
        "  while decoder_input.size(1) < seq_len:\n",
        "    # Decoder mask\n",
        "    decoder_mask = torch.tril(torch.ones(h, decoder_input.size(1), decoder_input.size(1))).unsqueeze(0).type(torch.int).to(device)\n",
        "    decoder_out = pg_model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
        "    prob = pg_model(decoder_out[:,-1])\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1)\n",
        "    txt_pred = tokenizer.decode(next_word)\n",
        "\n",
        "    if next_word == 102:\n",
        "      break\n",
        "\n",
        "src_list = decoder_input.tolist()\n",
        "decoded_texts = [tokenizer.decode(ids) for ids in src_list]\n",
        "print(decoded_texts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bPNm9s879LS"
      },
      "source": [
        "## Dimension check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HL4qw-lpVBT"
      },
      "source": [
        "### Check InputEmbedding & Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRpEJEiWy4rT",
        "outputId": "414cf729-1a26-4bda-a143-5be03753b31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target size input embedding: torch.Size([1, 2, 256])\n",
            "\n",
            "target size positional embedding: torch.Size([1, 2, 256])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tgt_embedding = InputEmbedding(vocab_size, d_model)\n",
        "tgt_pos_embedding = PositionalEmbedding(d_model, seq_len)\n",
        "\n",
        "\n",
        "x = tgt_embedding(decoder_input)\n",
        "print(f\"Decoder input size after Input Embedding: {x.size()}\\n\")\n",
        "\n",
        "x = tgt_pos_embedding(x)\n",
        "print(f\"Decoder input size after Positional Embedding: {x.size()}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg-x95NrDVcl"
      },
      "source": [
        "### Check Self-Head Attention part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKHELtWODUJS",
        "outputId": "a8bfd0b2-fdf6-46d2-ccdf-917a8aacc8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query size: torch.Size([1, 2, 256])\n",
            "\n",
            "key size: torch.Size([1, 2, 256])\n",
            "\n",
            "value size: torch.Size([1, 2, 256])\n",
            "\n",
            "query size: torch.Size([1, 2, 4, 64])\n",
            "\n",
            "key size: torch.Size([1, 2, 4, 64])\n",
            "\n",
            "value size: torch.Size([1, 2, 4, 64])\n",
            "\n",
            "query size: torch.Size([4, 2, 64])\n",
            "\n",
            "key size: torch.Size([4, 2, 64])\n",
            "\n",
            "value size: torch.Size([4, 2, 64])\n",
            "\n",
            "W before masking:\n",
            " tensor([[[-116.4165, -115.3759],\n",
            "         [-115.7520, -114.7065]],\n",
            "\n",
            "        [[ 210.6931,  210.8742],\n",
            "         [ 210.8873,  211.0660]],\n",
            "\n",
            "        [[-108.3419, -107.6617],\n",
            "         [-108.7478, -108.0781]],\n",
            "\n",
            "        [[ -45.7094,  -45.6179],\n",
            "         [ -45.4062,  -45.3125]]], grad_fn=<DivBackward0>)\n",
            "\n",
            "W size: torch.Size([4, 2, 2])\n",
            "\n",
            "Mask:\n",
            " tensor([[[1, 0],\n",
            "         [1, 1]],\n",
            "\n",
            "        [[1, 0],\n",
            "         [1, 1]],\n",
            "\n",
            "        [[1, 0],\n",
            "         [1, 1]],\n",
            "\n",
            "        [[1, 0],\n",
            "         [1, 1]]], dtype=torch.int32)\n",
            "\n",
            "Decoder mask size: torch.Size([4, 2, 2])\n",
            "\n",
            "W after masking:\n",
            " tensor([[[-1.1642e+02, -1.0000e+09],\n",
            "         [-1.1575e+02, -1.1471e+02]],\n",
            "\n",
            "        [[ 2.1069e+02, -1.0000e+09],\n",
            "         [ 2.1089e+02,  2.1107e+02]],\n",
            "\n",
            "        [[-1.0834e+02, -1.0000e+09],\n",
            "         [-1.0875e+02, -1.0808e+02]],\n",
            "\n",
            "        [[-4.5709e+01, -1.0000e+09],\n",
            "         [-4.5406e+01, -4.5313e+01]]], grad_fn=<MaskedFillBackward0>)\n",
            "\n",
            "out size: torch.Size([4, 2, 64])\n",
            "\n",
            "out size: torch.Size([1, 2, 256])\n",
            "\n",
            "Self Head Attention out size: torch.Size([1, 2, 256])\n",
            "\n",
            "x size: torch.Size([1, 2, 256])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ln1 = nn.LayerNorm(d_model)\n",
        "\n",
        "SHA_W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "SHA_W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "SHA_W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "SHA_W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "query = SHA_W_q(x)\n",
        "key = SHA_W_k(x)\n",
        "value = SHA_W_v(x)\n",
        "\n",
        "print(f\"query size: {query.size()}\\n\")\n",
        "print(f\"key size: {key.size()}\\n\")\n",
        "print(f\"value size: {value.size()}\\n\")\n",
        "\n",
        "q_B, q_seq_len, _ = query.size()\n",
        "k_B, k_seq_len, _ = key.size()\n",
        "v_B, v_seq_len, _ = value.size()\n",
        "# size: (Batch, Seq_len, d_model) -> (Batch, Seq_len, h, d_model // h)\n",
        "query = query.view(q_B, q_seq_len, h, d_k)\n",
        "key = key.view(k_B, k_seq_len, h, d_k)\n",
        "value = value.view(v_B, v_seq_len, h, d_k)\n",
        "print(f\"query size: {query.size()}\\n\")\n",
        "print(f\"key size: {key.size()}\\n\")\n",
        "print(f\"value size: {value.size()}\\n\")\n",
        "\n",
        "# size: (Batch, Seq_len, h, d_model // h) -> (B*h, seq_len, d_model//h)\n",
        "query = query.transpose(1,2).contiguous().view(q_B * h, q_seq_len, d_k)\n",
        "key = key.transpose(1,2).contiguous().view(k_B * h, k_seq_len, d_k)\n",
        "value = value.transpose(1,2).contiguous().view(v_B * h, v_seq_len, d_k)\n",
        "\n",
        "print(f\"query size: {query.size()}\\n\")\n",
        "print(f\"key size: {key.size()}\\n\")\n",
        "print(f\"value size: {value.size()}\\n\")\n",
        "\n",
        "W = query @ key.transpose(1, 2) / math.sqrt(d_k)\n",
        "print(f\"W before masking:\\n {W}\\n\")\n",
        "print(f\"W size: {W.size()}\\n\")\n",
        "\n",
        "decoder_mask = torch.tril(torch.ones(h, decoder_input.size(1), decoder_input.size(1))).unsqueeze(0).type(torch.int).to(device)\n",
        "decoder_mask = decoder_mask.view(q_B * h, q_seq_len, q_seq_len)\n",
        "print(f\"Mask:\\n {decoder_mask}\\n\")\n",
        "print(f\"Decoder mask size: {decoder_mask.size()}\\n\")\n",
        "W = W.masked_fill_(decoder_mask == 0, -1e9)\n",
        "print(f\"W after masking:\\n {W}\\n\")\n",
        "\n",
        "W = W.softmax(dim = -1)\n",
        "out = W @ value\n",
        "print(f\"out size: {out.size()}\\n\")\n",
        "B, seq_len, d_k = out.size()\n",
        "B = B //h\n",
        "out = out.view(B, h, seq_len, d_k)\n",
        "out = out.transpose(1, 2).contiguous().view(B, seq_len, h * d_k)\n",
        "print(f\"out size: {out.size()}\\n\")\n",
        "\n",
        "sha_out = SHA_W_o(out)\n",
        "print(f\"Self Head Attention out size: {sha_out.size()}\\n\")\n",
        "\n",
        "x = x + sha_out\n",
        "x = ln1(x)\n",
        "print(f\"x size: {x.size()}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoE9hJIWDfzk"
      },
      "source": [
        "Check Cross Head Attention part now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BraTa-xwDkHn",
        "outputId": "348b71f7-2ea8-428d-90fd-c472b041e90a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query size: torch.Size([1, 2, 256])\n",
            "\n",
            "key size: torch.Size([1, 256, 256])\n",
            "\n",
            "value size: torch.Size([1, 256, 256])\n",
            "\n",
            "query size: torch.Size([1, 2, 4, 64])\n",
            "\n",
            "key size: torch.Size([1, 256, 4, 64])\n",
            "\n",
            "value size: torch.Size([1, 256, 4, 64])\n",
            "\n",
            "query size: torch.Size([4, 2, 64])\n",
            "\n",
            "key size: torch.Size([4, 256, 64])\n",
            "\n",
            "value size: torch.Size([4, 256, 64])\n",
            "\n",
            "W before masking:\n",
            " tensor([[[ 3.9521e-01,  6.1659e-01,  3.4466e-01,  ...,  6.5345e-01,\n",
            "           6.5383e-01,  6.5523e-01],\n",
            "         [ 3.9477e-01,  6.1501e-01,  3.4370e-01,  ...,  6.5471e-01,\n",
            "           6.5510e-01,  6.5653e-01]],\n",
            "\n",
            "        [[-7.6491e-02, -3.0708e-01,  3.9608e-01,  ...,  1.2021e-01,\n",
            "           1.1990e-01,  1.2064e-01],\n",
            "         [-7.8172e-02, -3.0852e-01,  3.9700e-01,  ...,  1.1780e-01,\n",
            "           1.1749e-01,  1.1822e-01]],\n",
            "\n",
            "        [[-9.3075e-02, -3.9950e-02,  5.1671e-04,  ..., -1.7367e-01,\n",
            "          -1.7641e-01, -1.7916e-01],\n",
            "         [-8.3442e-02, -3.4631e-02,  8.4259e-03,  ..., -1.6870e-01,\n",
            "          -1.7146e-01, -1.7423e-01]],\n",
            "\n",
            "        [[ 1.4594e-01,  1.8281e-02,  2.7771e-01,  ...,  1.0425e-01,\n",
            "           1.0614e-01,  1.0603e-01],\n",
            "         [ 1.4589e-01,  1.8810e-02,  2.8180e-01,  ...,  1.0572e-01,\n",
            "           1.0762e-01,  1.0753e-01]]], grad_fn=<DivBackward0>)\n",
            "\n",
            "W size: torch.Size([4, 2, 256])\n",
            "\n",
            "Mask:\n",
            " tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]]])\n",
            "\n",
            "Source mask size: torch.Size([4, 2, 256])\n",
            "\n",
            "W after masking:\n",
            " tensor([[[ 3.9521e-01,  6.1659e-01,  3.4466e-01,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09],\n",
            "         [ 3.9477e-01,  6.1501e-01,  3.4370e-01,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "        [[-7.6491e-02, -3.0708e-01,  3.9608e-01,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09],\n",
            "         [-7.8172e-02, -3.0852e-01,  3.9700e-01,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "        [[-9.3075e-02, -3.9950e-02,  5.1671e-04,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09],\n",
            "         [-8.3442e-02, -3.4631e-02,  8.4259e-03,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "        [[ 1.4594e-01,  1.8281e-02,  2.7771e-01,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09],\n",
            "         [ 1.4589e-01,  1.8810e-02,  2.8180e-01,  ..., -1.0000e+09,\n",
            "          -1.0000e+09, -1.0000e+09]]], grad_fn=<MaskedFillBackward0>)\n",
            "\n",
            "out size: torch.Size([4, 2, 64])\n",
            "\n",
            "out size: torch.Size([1, 2, 256])\n",
            "\n",
            "Cross Head Attention out size: torch.Size([1, 2, 256])\n",
            "\n",
            "x size: torch.Size([1, 2, 256])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Encoder input size: torch.Size([1, 256])\n",
        "# Encoder mask size: torch.Size([1, 4, 256, 256])\n",
        "# Encoder output size: torch.Size([1, 256, 256])\n",
        "\n",
        "# Decoder input size: torch.Size([1, 1])\n",
        "\n",
        "ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "# Cross-Head Attention\n",
        "# CrossHeadAttention(x, encoder_out, encoder_out, src_mask)\n",
        "CHA_W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "CHA_W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "CHA_W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "CHA_W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "query = CHA_W_q(x)\n",
        "key = CHA_W_k(encoder_out)\n",
        "value = CHA_W_v(encoder_out)\n",
        "\n",
        "print(f\"query size: {query.size()}\\n\")\n",
        "print(f\"key size: {key.size()}\\n\")\n",
        "print(f\"value size: {value.size()}\\n\")\n",
        "\n",
        "q_B, q_seq_len, _ = query.size()\n",
        "k_B, k_seq_len, _ = key.size()\n",
        "v_B, v_seq_len, _ = value.size()\n",
        "# size: (Batch, Seq_len, d_model) -> (Batch, Seq_len, h, d_model // h)\n",
        "query = query.view(q_B, q_seq_len, h, d_k)\n",
        "key = key.view(k_B, k_seq_len, h, d_k)\n",
        "value = value.view(v_B, v_seq_len, h, d_k)\n",
        "print(f\"query size: {query.size()}\\n\")\n",
        "print(f\"key size: {key.size()}\\n\")\n",
        "print(f\"value size: {value.size()}\\n\")\n",
        "\n",
        "# size: (Batch, Seq_len, h, d_model // h) -> (B*h, seq_len, d_model//h)\n",
        "query = query.transpose(1,2).contiguous().view(q_B * h, q_seq_len, d_k)\n",
        "key = key.transpose(1,2).contiguous().view(k_B * h, k_seq_len, d_k)\n",
        "value = value.transpose(1,2).contiguous().view(v_B * h, v_seq_len, d_k)\n",
        "\n",
        "print(f\"query size: {query.size()}\\n\")\n",
        "print(f\"key size: {key.size()}\\n\")\n",
        "print(f\"value size: {value.size()}\\n\")\n",
        "\n",
        "W = query @ key.transpose(1, 2) / math.sqrt(d_k)\n",
        "print(f\"W before masking:\\n {W}\\n\")\n",
        "print(f\"W size: {W.size()}\\n\")\n",
        "\n",
        "encoder_mask = encoder_mask.view(k_B * h, k_seq_len, k_seq_len)\n",
        "encoder_mask = encoder_mask[:,:q_seq_len,:]\n",
        "print(f\"Mask:\\n {encoder_mask}\\n\")\n",
        "print(f\"Source mask size: {encoder_mask[:,:q_seq_len,:].size()}\\n\")\n",
        "W = W.masked_fill_(encoder_mask == 0, -1e9)\n",
        "print(f\"W after masking:\\n {W}\\n\")\n",
        "\n",
        "W = W.softmax(dim = -1)\n",
        "out = W @ value\n",
        "print(f\"out size: {out.size()}\\n\")\n",
        "B, seq_len, d_k = out.size()\n",
        "B = B //h\n",
        "out = out.view(B, h, seq_len, d_k)\n",
        "out = out.transpose(1, 2).contiguous().view(B, seq_len, h * d_k)\n",
        "print(f\"out size: {out.size()}\\n\")\n",
        "\n",
        "cha_out = CHA_W_o(out)\n",
        "print(f\"Cross Head Attention out size: {cha_out.size()}\\n\")\n",
        "\n",
        "x = x + cha_out\n",
        "x = ln2(x)\n",
        "print(f\"x size: {x.size()}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoHcmyAEL1x0"
      },
      "source": [
        "Feed Forward now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAcDelfHMpR7",
        "outputId": "afd4d77c-74ba-473b-d39a-d4764a07f8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x size: torch.Size([1, 2, 256])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ff = FeedForward(config)\n",
        "ln3 = nn.LayerNorm(d_model)\n",
        "\n",
        "x = x + ff(x)\n",
        "x = ln3(x)\n",
        "print(f\"x size: {x.size()}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05686f36ecd24d07bb33bab900d247ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f49f0627614c709ddce7e3a7ece74e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8e3bcc630a4af78664647b63cbb70d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2745096a7cf045fd8fc49798aa288b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3681938753284dfcb499759c1f385a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a1c1e6f89e64aee879d7d174c2fbc98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b30baca83534a80b6ea98aebb5f9d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "766f01ee880b45d4b6b82507275aedb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78979ab4cc49432a8dc213738679c987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b69457bc34b499d8a7f457991a51102": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05686f36ecd24d07bb33bab900d247ee",
            "placeholder": "​",
            "style": "IPY_MODEL_3b30baca83534a80b6ea98aebb5f9d99",
            "value": "Token is valid (permission: write)."
          }
        },
        "8015fabb22fb47da9f4ca79e50134a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e63d2c52fbe44fdf9d0eea5edd5c61ea",
            "style": "IPY_MODEL_2745096a7cf045fd8fc49798aa288b78",
            "tooltip": ""
          }
        },
        "83e3ff65e15845a8993a97c798b70f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bc60ed2782c44dab8e4ee59461cfe98",
            "placeholder": "​",
            "style": "IPY_MODEL_acf6f0c5d993451a97d3c11ec13b3aaa",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "860ecadfe5db4c2091efc4639460800e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a8e3bcc630a4af78664647b63cbb70d",
            "placeholder": "​",
            "style": "IPY_MODEL_dea53186d23643c9991b5c4f2c0b8c11",
            "value": "Login successful"
          }
        },
        "8b14ead53fec4b4e976f19a886eda194": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc60ed2782c44dab8e4ee59461cfe98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b6665729ab54898b443ea6b7201dd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f3c23b017c342909083c6d6c014111d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1818b7e6d5b4274b3aab52afa2d6b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b14ead53fec4b4e976f19a886eda194",
            "placeholder": "​",
            "style": "IPY_MODEL_766f01ee880b45d4b6b82507275aedb0",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "acf6f0c5d993451a97d3c11ec13b3aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2a125f819814fe69707a1bb023e3dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1bfc5a5876b40de9d87c7327305aaea",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f95ccb4a524c8491d6774164c298b6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b4aff983cdc64f81996062c778a3899e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f95ccb4a524c8491d6774164c298b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7e225094ced4d059e14dd8352ad52c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3a1c1e6f89e64aee879d7d174c2fbc98",
            "style": "IPY_MODEL_9b6665729ab54898b443ea6b7201dd5e",
            "value": true
          }
        },
        "c1fac369de064057873f4a0956320fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c6ed637ff9854abbaccad7681fafdf04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e0c39cafc43245589f1143eb5cb4a58d",
            "placeholder": "​",
            "style": "IPY_MODEL_9f3c23b017c342909083c6d6c014111d",
            "value": ""
          }
        },
        "d1bfc5a5876b40de9d87c7327305aaea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da48568fba184bbda81d738c70e864c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4aff983cdc64f81996062c778a3899e",
            "placeholder": "​",
            "style": "IPY_MODEL_78979ab4cc49432a8dc213738679c987",
            "value": "Connecting..."
          }
        },
        "dea53186d23643c9991b5c4f2c0b8c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0c39cafc43245589f1143eb5cb4a58d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e370322303f14a4f988db4a0706cc64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b69457bc34b499d8a7f457991a51102",
              "IPY_MODEL_f04df418bda8492a9c16f4c36ccb21a1",
              "IPY_MODEL_83e3ff65e15845a8993a97c798b70f46",
              "IPY_MODEL_860ecadfe5db4c2091efc4639460800e"
            ],
            "layout": "IPY_MODEL_c1fac369de064057873f4a0956320fc9"
          }
        },
        "e63d2c52fbe44fdf9d0eea5edd5c61ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04df418bda8492a9c16f4c36ccb21a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f49f0627614c709ddce7e3a7ece74e",
            "placeholder": "​",
            "style": "IPY_MODEL_3681938753284dfcb499759c1f385a16",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
